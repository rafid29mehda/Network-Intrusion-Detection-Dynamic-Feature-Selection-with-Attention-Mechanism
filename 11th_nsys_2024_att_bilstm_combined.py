# -*- coding: utf-8 -*-
"""11th NSys 2024_att_bilstm_combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ol154ClpSbeip6G2iVdLPRc_pClg8MSV
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Bidirectional, Attention, Multiply, Lambda
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, Flatten, MaxPooling1D, Dropout, Dense, Input

# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Load the combined dataset
file_path = 'combined_binary_classification_dataset.csv'
df = pd.read_csv(file_path)

# Preprocessing: Label encode categorical columns and scale numerical columns
label_encoder = LabelEncoder()
categorical_columns = ['Protocol_type', 'Service', 'Flag', 'attack_type']

# Apply label encoding
for col in categorical_columns:
    df[col] = label_encoder.fit_transform(df[col])

# Separate features and target variable
X = df.drop(columns=['attack_type'])
y = df['attack_type']

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 1: Split the dataset into train (60%), test (20%), and validation (20%)
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.4, random_state=42)
X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Reshape X_train for Attention-based BiLSTM model
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)

# Define the Attention-based BiLSTM model
def attention_bilstm_model(input_shape):
    inputs = Input(shape=(input_shape[0], input_shape[1]))

    # Bi-directional LSTM layer
    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(inputs)

    # Attention mechanism
    attention = Attention()([lstm_out, lstm_out])
    attention_out = Multiply()([attention, lstm_out])

    # Sum across time steps (features)
    attention_out = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attention_out)

    # Fully connected output layer
    outputs = Dense(1, activation='sigmoid')(attention_out)

    # Define the model
    model = Model(inputs, outputs)
    return model

# Create the model
input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])
attention_model = attention_bilstm_model(input_shape)

# Compile the model
attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the Attention-based BiLSTM model
attention_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=1)

# Extract attention scores
attention_layer = attention_model.layers[-4]  # Layer containing the attention scores
attention_extractor_model = Model(inputs=attention_model.input, outputs=attention_layer.output)

# Predict attention scores
attention_scores = attention_extractor_model.predict(X_train_reshaped)

# Get feature names
feature_names = X.columns

# Average attention scores across samples (axis=0) and time steps (axis=1)
average_attention_scores = np.mean(attention_scores, axis=(0, 1))

# Combine feature names with their corresponding attention scores
feature_attention_scores = list(zip(feature_names, average_attention_scores))

# Print the attention scores for all features
print("Attention Scores for All Features:")
for feature, score in feature_attention_scores:
    print(f"{feature}: {score}")

# Sort the features based on their attention scores in descending order
sorted_feature_attention_scores = sorted(feature_attention_scores, key=lambda x: x[1], reverse=True)

# Select features with attention scores greater than 0
selected_features = [feature for feature, score in sorted_feature_attention_scores if score > 0]

import matplotlib.pyplot as plt

# Sort the features based on their attention scores in descending order
sorted_feature_attention_scores = sorted(feature_attention_scores, key=lambda x: x[1], reverse=True)

# Separate features and scores for plotting
features = [feature for feature, score in sorted_feature_attention_scores]
scores = [score for feature, score in sorted_feature_attention_scores]

# Print the sorted attention scores
print("Sorted Attention Scores:")
for feature, score in sorted_feature_attention_scores:
    print(f"{feature}: {score}")

# Plotting
plt.figure(figsize=(12, 8))
bars = plt.barh(features, scores, color=['skyblue' if score > 0 else 'lightcoral' for score in scores])
plt.xlabel('Attention Scores')
plt.ylabel('Features')
plt.title('Feature Importance Based on Attention Scores (Positive and Negative)')

# Display the highest scores at the top
plt.gca().invert_yaxis()

plt.show()

# Subset the dataset to include only the selected features
X_train_selected = pd.DataFrame(X_train, columns=X.columns)[selected_features]
X_test_selected = pd.DataFrame(X_test, columns=X.columns)[selected_features]
X_val_selected = pd.DataFrame(X_val, columns=X.columns)[selected_features]

# Standardize the selected features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)
X_val_scaled = scaler.transform(X_val_selected)

# Train and evaluate Random Forest model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train_scaled, y_train)
rf_test_predictions = rf_model.predict(X_test_scaled)
rf_val_predictions = rf_model.predict(X_val_scaled)

print("Random Forest Classification Report (Test):")
print(classification_report(y_test, rf_test_predictions))

print("Random Forest Classification Report (Validation):")
print(classification_report(y_val, rf_val_predictions))

# Train and evaluate SVM model
svm_model = SVC(random_state=42)
svm_model.fit(X_train_scaled, y_train)
svm_test_predictions = svm_model.predict(X_test_scaled)
svm_val_predictions = svm_model.predict(X_val_scaled)

print("SVM Classification Report (Test):")
print(classification_report(y_test, svm_test_predictions))

print("SVM Classification Report (Validation):")
print(classification_report(y_val, svm_val_predictions))

# Train and evaluate XGBoost model
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train_scaled, y_train)
xgb_test_predictions = xgb_model.predict(X_test_scaled)
xgb_val_predictions = xgb_model.predict(X_val_scaled)

print("XGBoost Classification Report (Test):")
print(classification_report(y_test, xgb_test_predictions))

print("XGBoost Classification Report (Validation):")
print(classification_report(y_val, xgb_val_predictions))

# Reshape the data for CNN
X_train_cnn = X_train_scaled.reshape(X_train_scaled.shape[0], X_train_scaled.shape[1], 1)
X_test_cnn = X_test_scaled.reshape(X_test_scaled.shape[0], X_test_scaled.shape[1], 1)
X_val_cnn = X_val_scaled.reshape(X_val_scaled.shape[0], X_val_scaled.shape[1], 1)

# Build and train CNN model
cnn_model = Sequential()
cnn_model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))
cnn_model.add(MaxPooling1D(pool_size=2))
cnn_model.add(Dropout(0.2))
cnn_model.add(Flatten())
cnn_model.add(Dense(100, activation='relu'))
cnn_model.add(Dense(1, activation='sigmoid'))

# Compile and train the CNN model
cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
cnn_model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, verbose=1)

# Evaluate the CNN model on test and validation data
cnn_test_predictions = cnn_model.predict(X_test_cnn)
cnn_val_predictions = cnn_model.predict(X_val_cnn)

cnn_test_predictions = (cnn_test_predictions > 0.5).astype(int).reshape(-1)  # Convert to binary output
cnn_val_predictions = (cnn_val_predictions > 0.5).astype(int).reshape(-1)

print("CNN Classification Report (Test):")
print(classification_report(y_test, cnn_test_predictions))

print("CNN Classification Report (Validation):")
print(classification_report(y_val, cnn_val_predictions))

import time

# Define the Attention-based BiLSTM model
def attention_bilstm_model(input_shape):
    inputs = Input(shape=(input_shape[0], input_shape[1]))

    # Bi-directional LSTM layer
    lstm_out = Bidirectional(LSTM(64, return_sequences=True))(inputs)

    # Attention mechanism
    attention = Attention()([lstm_out, lstm_out])
    attention_out = Multiply()([attention, lstm_out])

    # Sum across time steps (features)
    attention_out = Lambda(lambda x: tf.reduce_sum(x, axis=1))(attention_out)

    # Fully connected output layer
    outputs = Dense(1, activation='sigmoid')(attention_out)

    # Define the model
    model = Model(inputs, outputs)
    return model

# Create the model
input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])
attention_model = attention_bilstm_model(input_shape)

# Compile the model
attention_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Measure the training time
start_time = time.time()

# Train the Attention-based BiLSTM model
attention_model.fit(X_train_reshaped, y_train, epochs=10, batch_size=32, verbose=1)

# Calculate the training time
training_time = time.time() - start_time
print(f"Training time for Attention-based BiLSTM model: {training_time:.2f} seconds")

# Extract attention scores
start_time = time.time()

attention_layer = attention_model.layers[-4]  # Layer containing the attention scores
attention_extractor_model = Model(inputs=attention_model.input, outputs=attention_layer.output)

# Predict attention scores
attention_scores = attention_extractor_model.predict(X_train_reshaped)

# Calculate the time to extract attention scores
extraction_time = time.time() - start_time
print(f"Time to extract attention scores: {extraction_time:.2f} seconds")

from memory_profiler import memory_usage

# Measure the memory usage and training time
start_time = time.time()
mem_usage = memory_usage((attention_model.fit, (X_train_reshaped, y_train), {'epochs': 10, 'batch_size': 32, 'verbose': 1}))
training_time = time.time() - start_time

print(f"Training time for Attention-based BiLSTM model: {training_time:.2f} seconds")
print(f"Maximum memory usage during training: {max(mem_usage):.2f} MiB")

